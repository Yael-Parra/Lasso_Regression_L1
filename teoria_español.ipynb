{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# <span style=\"color:#ffc509;\"> **Regresi√≥n LASSO L1** (Operador de Selecci√≥n y Contracci√≥n Menor Absolutor) </span>\n",
    "\n",
    "</div>\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresi√≥n Lasso es una t√©cnica estad√≠stica utilizada en el aprendizaje autom√°tico para mejorar la precisi√≥n de los modelos predictivos, especialmente cuando se trabaja con grandes conjuntos de datos. Su principal objetivo es prevenir el sobreajuste, un problema que ocurre cuando un modelo se ajusta demasiado a los datos de entrenamiento y no funciona bien con datos nuevos. Lasso lo logra mediante la aplicaci√≥n de una penalizaci√≥n que reduce la magnitud de los coeficientes de las variables predictoras, incluso forzando algunos a cero. Esto simplifica el modelo, selecciona autom√°ticamente las variables m√°s importantes y mejora su capacidad de generalizaci√≥n.\n",
    "\n",
    "<i>\n",
    "Aclaraci√≥n:\n",
    "Aunque la regresi√≥n Lasso y la regularizaci√≥n L1 suelen mencionarse como si fueran lo mismo, en realidad no lo son. Lasso es un modelo de regresi√≥n que utiliza la penalizaci√≥n L1, pero la regularizaci√≥n L1 tambi√©n se emplea en otros modelos m√°s all√° de la regresi√≥n, como en regresi√≥n log√≠stica, m√°quinas de soporte vectorial (SVM), redes neuronales y selecci√≥n de caracter√≠sticas en modelos lineales. La confusi√≥n ocurre porque Lasso es el ejemplo m√°s com√∫n de regularizaci√≥n L1, lo que lleva a que muchas fuentes los traten como sin√≥nimos. </i>\n",
    "\n",
    "### <span style=\"color:#ffc509\"> ¬øQu√© es la **regularizaci√≥n?** </span>\n",
    "\n",
    "En el aprendizaje autom√°tico, la regularizaci√≥n es como darle un freno al modelo para que no se vuelva demasiado complicado. El problema es que, a veces, el modelo se aprende de memoria los datos de entrenamiento, como un estudiante que memoriza respuestas exactas para un examen. Esto se llama \"sobreajuste\". Cuando esto pasa, el modelo funciona mal con datos nuevos. La regularizaci√≥n a√±ade una \"penalizaci√≥n\" a la formula que el modelo usa para saber cuando se equivoca, para que aprenda de manera m√°s general, y no tan especificamente, y de esta manera, no se memorice los datos de entrenamiento.\n",
    "\n",
    "### <span style=\"color:#ffc509\"> **¬øQu√©** es la regresi√≥n Lasso? </span>\n",
    "\n",
    "La regresi√≥n Lasso, tambi√©n conocida como regularizaci√≥n L1, es un tipo de regresi√≥n lineal que a√±ade una penalizaci√≥n basada en el valor absoluto de los coeficientes de las variables predictoras. Esta penalizaci√≥n fuerza a algunos coeficientes a ser exactamente cero, lo que significa que esas variables se eliminan del modelo.\n",
    "\n",
    "### <span style=\"color:#ffc509\"> **¬øPor qu√©** se utiliza regresi√≥n Lasso? </span>\n",
    "\n",
    "- Prevenci√≥n del sobreajuste: Reduce la complejidad del modelo, evitando que se ajuste demasiado a los datos de entrenamiento.\n",
    "- Selecci√≥n autom√°tica de variables: Identifica y retiene solo las variables m√°s relevantes para la predicci√≥n, simplificando el modelo y mejorando su interpretabilidad.\n",
    "- Manejo de multicolinealidad: Aunque no la elimina por completo, Lasso puede mitigar los efectos de la multicolinealidad (alta correlaci√≥n entre variables predictoras) al seleccionar una variable y eliminar las correlacionadas\n",
    "\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **¬øC√≥mo** funciona la regresi√≥n LASSO? </span>\n",
    "\n",
    "1.  Penalizaci√≥n L1: Lasso a√±ade un t√©rmino de penalizaci√≥n a la funci√≥n de coste del modelo de regresi√≥n lineal. Este t√©rmino es proporcional a la suma de los valores absolutos de los coeficientes de las variables predictoras.\n",
    "2. Par√°metro Lambda (Œª): Este par√°metro controla la fuerza de la penalizaci√≥n.\n",
    "    - Un Œª alto aumenta la penalizaci√≥n, lo que reduce m√°s coeficientes a cero, simplificando el modelo.\n",
    "    - Un Œª bajo disminuye la penalizaci√≥n, conservando m√°s variables en el modelo.\n",
    "3. Minimizaci√≥n del Error Cuadr√°tico Medio (MSE): Lasso busca el valor de los coeficientes que minimiza el MSE (la diferencia entre los valores predichos y los reales), sujeto a la penalizaci√≥n L1.\n",
    "4. Selecci√≥n de Variables: Al forzar algunos coeficientes a cero, Lasso realiza una selecci√≥n autom√°tica de variables, qued√°ndose solo con las m√°s importantes.\n",
    "\n",
    "\n",
    "La funci√≥n de costo que la regresi√≥n LASSO trata de minimizar es:\n",
    "\n",
    "$$ J(\\beta) = \\text{Error} + \\lambda \\sum_{i=1}^{p} |\\beta_i| $$\n",
    "\n",
    "Donde:\n",
    "\n",
    "* $J(\\beta)$ es la funci√≥n de costo.\n",
    "* $\\text{Error}$ es una medida del error entre los valores predichos y los valores reales (por ejemplo, la suma de los errores al cuadrado en la regresi√≥n lineal est√°ndar).\n",
    "* $\\lambda$ (lambda) es el **par√°metro de regularizaci√≥n** (un hiperpar√°metro que se ajusta). Controla la fuerza de la penalizaci√≥n. Cuanto mayor sea el valor de $\\lambda$, mayor ser√° la penalizaci√≥n.\n",
    "* $\\sum_{i=1}^{p} |\\beta_i|$ es la **norma L1** de los coeficientes, que es la suma de los valores absolutos de todos los coeficientes ($\\beta_i$).\n",
    "* $p$ es el n√∫mero de variables predictoras.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Cu√°ndo** se utiliza la regresi√≥n Lasso? </span>\n",
    "\n",
    "- Conjuntos de datos de alta dimensi√≥n: Cuando hay muchas m√°s variables predictoras que observaciones.\n",
    "- Selecci√≥n autom√°tica de variables: Cuando se necesita identificar las variables m√°s relevantes para la predicci√≥n.\n",
    "- Problemas predictivos: Donde el objetivo principal es la precisi√≥n de la predicci√≥n.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **La clave de la regresi√≥n LASSO: Selecci√≥n de caracter√≠sticas** </span>\n",
    "\n",
    "La principal caracter√≠stica distintiva de la regularizaci√≥n L1 (y por lo tanto de la regresi√≥n LASSO) es su capacidad para **forzar algunos de los coeficientes de las variables predictoras a ser exactamente cero**.\n",
    "\n",
    "* Cuando $\\lambda$ es suficientemente grande, la penalizaci√≥n L1 puede hacer que los coeficientes de las variables menos importantes se reduzcan a cero, eliminando efectivamente esas variables del modelo.\n",
    "* Esto convierte a la regresi√≥n LASSO en un m√©todo √∫til para la **selecci√≥n de caracter√≠sticas**, ya que identifica autom√°ticamente las variables m√°s relevantes para la predicci√≥n.\n",
    "* El modelo resultante es m√°s **parsimonioso** (tiene menos variables) y, por lo tanto, a menudo m√°s f√°cil de interpretar.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Diferencias Clave entre Regresi√≥n LASSO (L1) y Regresi√≥n Ridge (L2)** </span>\n",
    "\n",
    "| Caracter√≠stica          | Regresi√≥n LASSO (L1)                                      | Regresi√≥n Ridge (L2)                                           |\n",
    "|-------------------------|-----------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Tipo de Regularizaci√≥n** | Norma L1: Suma del valor absoluto de los coeficientes ($|\\beta_i|$) | Norma L2: Suma del cuadrado de los coeficientes ($\\beta_i^2$) |\n",
    "| **Efecto en Coeficientes** | Puede reducir algunos coeficientes a **exactamente cero**.  | Reduce la magnitud de los coeficientes, pero **raramente a cero**. |\n",
    "| **Selecci√≥n de Caracter√≠sticas** | **Realiza selecci√≥n de caracter√≠sticas** al eliminar variables. | **No realiza una selecci√≥n de caracter√≠sticas expl√≠cita**.      |\n",
    "| **Parsimonia del Modelo** | Tiende a generar modelos **m√°s parsimoniosos** (menos variables). | Tiende a mantener todas las variables en el modelo (aunque con pesos peque√±os). |\n",
    "| **Interpretabilidad** | Puede mejorar la **interpretabilidad** al simplificar el modelo. | La interpretabilidad puede ser menor debido a la presencia de todas las variables. |\n",
    "| **Manejo de Multicolinealidad** | Tiende a seleccionar una variable de un grupo correlacionado. | Distribuye el peso entre las variables correlacionadas.          |\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Ventajas** de la regresi√≥n LASSO: </span>\n",
    "\n",
    "* Selecci√≥n de caracter√≠sticas autom√°tica: IIdentifica y elimina variables irrelevantes, simplificando el modelo y facilitando su interpretaci√≥n. Por ejemplo, al predecir el precio de una casa, LASSO selecciona las caracter√≠sticas m√°s relevantes como el tama√±o y la ubicaci√≥n.\n",
    "* Ayuda a prevenir el sobreajuste: Reduce la complejidad del modelo al penalizar los coeficientes grandes, lo que mejora la precisi√≥n en datos nuevos.\n",
    "* √ötil en conjuntos de datos de alta dimensi√≥n: Funciona bien cuando hay muchas variables predictoras (alta dimensi√≥n), algunas de las cuales pueden ser irrelevantes.\n",
    "* Mejora la interpretabilidad del modelo: Al tener menos variables, el modelo es m√°s f√°cil de entender y explicar.\n",
    "* Puede manejar cierta multicolinealidad: Tiende a seleccionar una variable de un grupo de variables altamente correlacionadas y a establecer los coeficientes de las otras en cero. Por ejemplo, si tienes variables como \"metros cuadrados\" y \"n√∫mero de habitaciones\", LASSO selecciona la m√°s importante.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Desventajas** de la regresi√≥n LASSO: </span>\n",
    "\n",
    "* Puede descartar variables relevantes: Si $\\lambda$ es demasiado grande,  podr√≠a eliminar variables que realmente tienen un impacto en la predicci√≥n, lo que reducir√≠a la precisi√≥n del modelo.\n",
    "* Selecci√≥n arbitraria en alta multicolinealidad: Si hay grupos de variables muy correlacionadas, LASSO puede seleccionar una de ellas arbitrariamente(al azar), lo que puede ser inestable, ya que peque√±as variaciones en los datos podr√≠an cambiar la variable seleccionada.\n",
    "* Puede no funcionar tan bien como Ridge si todas las variables son relevantes: Si la mayor√≠a de las variables tienen alg√∫n impacto en la predicci√≥n, la regresi√≥n Ridge podr√≠a dar mejores resultados en t√©rminos de precisi√≥n predictiva.\n",
    "* La elecci√≥n del par√°metro $\\lambda$ es crucial: Un valor incorrecto de $\\lambda$ puede llevar a un modelo subajustado (si $\\lambda$ es demasiado grande) o sobreajustado (si $\\lambda$ es demasiado peque√±o). La selecci√≥n √≥ptima de $\\lambda$ a menudo se realiza mediante t√©cnicas de validaci√≥n cruzada.\n",
    "\n",
    "\n",
    "### <span style=\"color:#ffc509\"> Casos de Uso Comunes </span>\n",
    "\n",
    "- Gen√≥mica: Identificaci√≥n de genes relevantes para ciertas enfermedades.\n",
    "- Marketing: Selecci√≥n de las variables demogr√°ficas y de comportamiento m√°s importantes para predecir la respuesta a una campa√±a publicitaria.\n",
    "- Finanzas: Construcci√≥n de modelos de riesgo crediticio, seleccionando los factores m√°s influyentes.\n",
    "\n",
    "### <span style=\"color:#ffc509\"> **Diccionario** de T√©rminos Clave </span>\n",
    "\n",
    "- **Sobreajuste** (**Overfitting**): Un modelo que se ajusta demasiado a los datos de entrenamiento y no generaliza bien a datos nuevos.\n",
    "- **Regularizaci√≥n**: T√©cnicas para prevenir el sobreajuste, a√±adiendo informaci√≥n adicional para evitar la complejidad excesiva del modelo.\n",
    "- **Penalizaci√≥n L1**: La penalizaci√≥n utilizada en Lasso, basada en el valor absoluto de los coeficientes.\n",
    "- **Lambda** (**Œª**): El par√°metro que controla la fuerza de la penalizaci√≥n en Lasso.\n",
    "- **Alfa** (**Œ±**): Este t√©rmino no se menciona en el texto proporcionado.\n",
    "- **Multicolinealidad**: Alta correlaci√≥n entre variables predictoras.\n",
    "- **Error Cuadr√°tico Medio** (**MSE**): Una medida de la diferencia entre los valores predichos y los reales.\n",
    "- **Sesgo**: La diferencia entre las predicciones promedio de un modelo y los valores reales.\n",
    "- **Varianza**: La variabilidad de las predicciones de un modelo para diferentes conjuntos de datos.\n",
    "- **Funci√≥n de Coste**: Una funci√≥n que mide qu√© tan bien un modelo se ajusta a los datos; el objetivo es minimizar esta funci√≥n.\n",
    "- **Coeficientes Beta** (**Œ≤**): Los valores que multiplican las variables predictoras en un modelo de regresi√≥n.\n",
    "- **R2**: Una medida de qu√© proporci√≥n de la varianza en la variable dependiente es explicada por las variables independientes.\n",
    "- **Variable Dependiente**: La variable que se est√° prediciendo.\n",
    "- **Variables Independientes**: Las variables utilizadas para hacer predicciones.\n",
    "- **Elastic Net**: Una t√©cnica de regularizaci√≥n que combina las penalizaciones L1 (Lasso) y L2 (Ridge).\n",
    "- **Ridge**: Tambi√©n conocida como regularizaci√≥n L2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "# <span style=\"color:#ffc509\">  **Ejemplo**: ‚òï </span>\n",
    "\n",
    "</div>\n",
    "\n",
    "### Tenemos una cafeter√≠a y quieremos saber qu√© cosas hacen que venda m√°s caf√© por d√≠a.\n",
    "\n",
    "Hemos estado anotando datos durante varias semanas, y tenemos variables como:\n",
    "\n",
    "- ‚úÖ Hace fr√≠o\n",
    "- ‚úÖ Es fin de semana\n",
    "- ‚úÖ Hay promociones\n",
    "- ‚úÖ Hay m√∫sica en vivo\n",
    "- ‚úÖ Se encendieron luces decorativas\n",
    "- ‚úÖ Cambio en el men√∫\n",
    "- ‚úÖ Hay sillas nuevas\n",
    "- ‚úÖ Hubo luna llena üåï\n",
    "- ‚úÖ El gato se subi√≥ al mostrador üò∫\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Nuestro objetivo:\n",
    "**Predecir cu√°ntos caf√©s se vender√°n**, dependiendo de nuestros factores factores.\n",
    "\n",
    "Pero hay un problema:\n",
    "\n",
    "üëâ No todos esos factores realmente influyen.\n",
    "Algunos son importantes (como promociones), otros son ruido (como la luna llena o el gato üò∫).\n",
    "\n",
    "---\n",
    "\n",
    "### üìâ Aqu√≠ entra la regresi√≥n Lasso (L1)\n",
    "\n",
    "Lasso es un tipo de **regresi√≥n lineal con regularizaci√≥n**.\n",
    "Es como un filtro que te ayuda a:\n",
    "\n",
    "- üß† Seleccionar solo las variables que realmente ayudan a predecir\n",
    "- üßπ Eliminar o ignorar las que no sirven\n",
    "- üßæ Hacer el modelo m√°s simple y m√°s generalizable\n",
    "\n",
    "---\n",
    "\n",
    "### üìä ¬øC√≥mo funciona en la cafeter√≠a?\n",
    "\n",
    "Digamos que alimentamos al modelo con registros de muchos d√≠as:\n",
    "\n",
    "- D√≠a 1: Hace fr√≠o + fin de semana + promoci√≥n + m√∫sica = 280 caf√©s\n",
    "- D√≠a 2: Calor + lunes + sin promo + gato en el mostrador = 90 caf√©s\n",
    "- D√≠a 3: Fr√≠o + cambio de men√∫ + luces + m√∫sica = 200 caf√©s\n",
    "- D√≠a 4: Luna llena + sillas nuevas + promo = 100 caf√©s\n",
    "\n",
    "El modelo va **aprendiendo qu√© cosas realmente impactan en las ventas**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Modelo cl√°sico vs. Lasso\n",
    "\n",
    "### üßÆ Modelo de regresi√≥n lineal (sin penalizaci√≥n):\n",
    "$$ \\hat{y} = \\beta_0 + \\beta_1 \\cdot \\text{Fr√≠o} + \\beta_2 \\cdot \\text{FinDeSemana} + \\beta_3 \\cdot \\text{Promoci√≥n} + \\beta_4 \\cdot \\text{M√∫sica} + \\beta_5 \\cdot \\text{Luces} + \\beta_6 \\cdot \\text{Men√∫} + \\beta_7 \\cdot \\text{Sillas} + \\beta_8 \\cdot \\text{LunaLlena} + \\beta_9 \\cdot \\text{Gato} $$\n",
    "\n",
    "$\\hat{y}$ corresponde a las ventas predichas.\n",
    "\n",
    "Este modelo podr√≠a dar pesos ($\\beta_i$) peque√±os pero no exactamente cero a variables irrelevantes como la luna llena o el gato.\n",
    "\n",
    "### ü™Ñ Regresi√≥n Lasso (con **penalizaci√≥n L1**):\n",
    "\n",
    "$$ \\min_{\\beta} \\left\\{ \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\} $$\n",
    "\n",
    "\n",
    "que ser√≠a: \n",
    "\n",
    "$$ \\text{Coste} = \\underbrace{\\frac{1}{2n} \\sum_{i=1}^{n} (\\text{Ventas}_i - \\text{VentasPredichas}_i)^2}_{\\text{Qu√© tan lejos est√°n nuestras predicciones de las ventas reales}} + \\underbrace{\\lambda \\sum_{j=1}^{p} |\\beta_j|}_{\\text{Castigo por dar importancia a demasiados factores}} $$\n",
    "\n",
    "\n",
    "$ \\min_{\\beta}$ = El valor que queremos minimizar\n",
    "\n",
    "\n",
    "$ \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij} \\beta_j)^2 $ = **SSE** (suma de errores cuadr√°ticos) Suma los t√©rminos que le siguen para cada observaci√≥n $_i$, desde la primera ($_i$=1) hasta la √∫ltima ($_i$=n).\n",
    "\n",
    "- $y_i $ = El n√∫mero real de caf√©s vendidos en el d√≠a $_i$\n",
    "- $\\beta_0 $ = El n√∫mero que el modelo predice para el d√≠a $_i$\n",
    "\n",
    "- $\\sum_{j=1}^{p} x_{ij} \\beta_j$ = contribuci√≥n de todas las variables donde para una observaci√≥n espec√≠fica $_i$ , multiplica el valor de cada variable predictora ${j}$ $x_{ij}$ por su correspondiente coeficiente $\\beta_j$ y luego suma todos estos productos para todas las $^{p}$ variables predictoras.\n",
    "\n",
    "    - $\\sum$ = sumatoria\n",
    "    - ${}^{p}_{j=1}$  = Son los l√≠mites de la sumatoria\n",
    "        - ${j}$ = √≠ndice de la sumatoria\n",
    "        - =1 = indica que la sumatoria comienza con el valor de ${j}$ igual a 1 (primera variable predictora del conjunto de datos)\n",
    "        - $^{p}$ = indica que la sumatoria termina cuando el valor de ${j}$ alcanza ${p}$ . En donde ${p}$ representa el n√∫mero total de variables predictoras en el modelo\n",
    "    - $x_{ij}$ = representa el valor de la ${j}$ -√©sima variable predictora para la ${i}$ -√©sima observaci√≥n\n",
    "    - $\\beta_j$ = beta con sub√≠ndice ${j}$ representa el coeficiente asociado a la ${j}$ -√©sima variable predictora.\n",
    "    - $^{2}$ = el error es elevado al cuadrado para: 1.- evitar la cancelaci√≥n de errores positivos y negativos. 2.- Penaliza m√°s los errores grandes. 3.- Facilita el c√°lculo\n",
    "\n",
    "$\\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $ = **MSE** (error cuadr√°tico medio). C√≥mo la SSE pero a√±adiendo el factor escala que mide qu√© tan lejos est√°n nuestras predicciones de los valores reales\n",
    "\n",
    "- $y_i$ = Valor real de la variable dependiente (n√∫meros de caf√©s vendidos)\n",
    "- $\\hat{y}_i$ = Valor predicho de la variable dependiente por el modelo de regresi√≥n para la $_i$ √©sima observaci√≥n (n√∫meros de caf√©s que el modelo predice que se vender√°n el d√≠a $_i$)\n",
    "- $^{2}$ = El error es elevado al cuadrado para: 1.- evitar la cancelaci√≥n de errores positivos y negativos. 2.- Penaliza m√°s los errores grandes. 3.- Facilita el c√°lculo\n",
    "\n",
    "$\\lambda \\sum_{j=1}^{p} |\\beta_j|$ = es la **penalizaci√≥n L1**. Esta penalizaci√≥n fuerza a los coeficientes de las variables menos importantes a ser exactamente **cero**.\n",
    "- $\\lambda$ = Par√°metro de regularizaci√≥nn o constante de penalizaci√≥n\n",
    "    - Si es cercano a 0, la penalizaci√≥n es d√©bil (sin mucha tendencia a reducir los coeficientes a cero)\n",
    "    - Si es grande, la penalizaci√≥n es fuerte (modelo incentivado a hacer muchos de los coeficientes de las variables predictoras cero)\n",
    "    - Valor √≥ptimo : Se encuentra generalmente mediante t√©cnicas de validaci√≥n cruzada.\n",
    "- $\\sum$ = sumatoria\n",
    "    - ${}^{p}_{j=1}$  = Son los l√≠mites de la sumatoria\n",
    "        - ${j}$ = √≠ndice de la sumatoria\n",
    "        - =1 = indica que la sumatoria comienza con el valor de ${j}$ igual a 1 (primera variable predictora del conjunto de datos)\n",
    "        - $^{p}$ = indica que la sumatoria termina cuando el valor de ${j}$ alcanza ${p}$ . En donde ${p}$ representa el n√∫mero total de variables predictoras en el modelo\n",
    "- $\\beta_j$ = Coeficiente (el peso) asociado con la ${j}$ -√©sima variable predictora del modelo. Indica la fuerza y direcci√≥n de la relaci√≥n entre las variables predictora y dependiente (impacto considerable o no en la venta de caf√©).\n",
    "- $|\\cdot|$ = S√≠mbolo de valor absoluto, es decir, se toma el valor (n√∫mero) absoluto independientemente de si es negativo o positivo (-3=3, 3=3)\n",
    "---\n",
    "\n",
    "### ‚ú® El resultado m√°gico:\n",
    "\n",
    "Despu√©s de entrenar el modelo Lasso con los datos, podr√≠amos obtener algo como:\n",
    "\n",
    "$$ \\text{Ventas} \\approx 50 + 80 \\cdot \\text{Fr√≠o} + 120 \\cdot \\text{FinDeSemana} + 150 \\cdot \\text{Promoci√≥n} + 60 \\cdot \\text{M√∫sica} + 0 \\cdot \\text{Luces} + 30 \\cdot \\text{Men√∫} + 0 \\cdot \\text{Sillas} + 0 \\cdot \\text{LunaLlena} + 0 \\cdot \\text{Gato} $$\n",
    "\n",
    "Y esto nos dice claramente que:\n",
    "\n",
    "- üå°Ô∏è El **fr√≠o**, los **fines de semana** y las **promociones** son factores clave que aumentan tus ventas.\n",
    "- üé∂ La **m√∫sica en vivo** y un **nuevo men√∫** tienen un impacto menor pero positivo.\n",
    "- üí° Las **luces decorativas**, las **sillas nuevas**, la **luna llena** y las travesuras de tu **gato** parecen no tener una influencia significativa en las ventas.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ En resumen, Lasso nos ayuda a:\n",
    "\n",
    "- **Identificar los factores de venta reales.**\n",
    "- **Simplificar tu modelo de predicci√≥n.**\n",
    "- **Tomar decisiones m√°s inteligentes para tu cafeter√≠a.**\n",
    "\n",
    "con esto, ahora s√≠ nos podemos enfocar en los factores que realmente influyen en nuestro negocio haci√©ndolo crecer! ‚òï\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <span style=\"color:#ffc509\">  **Links √∫tiles** </span>\n",
    "\n",
    "https://www.ibm.com/es-es/think/topics/lasso-regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
