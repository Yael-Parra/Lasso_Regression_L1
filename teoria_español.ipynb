{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# <span style=\"color:#ffc509;\"> **Regresión Lasso L1** (Least Absolute Shrinkage and Selection Operator) </span>\n",
    "\n",
    "</div>\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de regresión LASSO (Least Absolute Shrinkage and Selection Operator), también conocido como regresión L1, es una técnica de regresión lineal que realiza tanto la selección de características como la regularización para mejorar la precisión predictiva e interpretabilidad de un modelo estadístico.\n",
    "\n",
    "### <span style=\"color:#ffc509\"> **¿Qué es la regularización?** </span>\n",
    "\n",
    "En el contexto del aprendizaje automático, la regularización es un conjunto de técnicas utilizadas para prevenir el sobreajuste (overfitting) en los modelos. El sobreajuste ocurre cuando un modelo aprende demasiado bien los datos de entrenamiento, incluyendo el ruido, y por lo tanto, tiene un mal desempeño con datos nuevos o no vistos. La regularización añade una \"penalización\" a la función de costo del modelo, lo que desalienta el aprendizaje de modelos demasiado complejos con coeficientes (pesos) muy grandes.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **¿Cómo funciona la regresión LASSO (L1)?** </span>\n",
    "\n",
    "La regresión LASSO utiliza la **regularización L1**. Esto significa que añade a la función de costo una penalización proporcional a la **suma del valor absoluto** de las magnitudes de los coeficientes de las variables predictoras.\n",
    "\n",
    "La función de costo que la regresión LASSO trata de minimizar es:\n",
    "\n",
    "$$ J(\\beta) = \\text{Error} + \\lambda \\sum_{i=1}^{p} |\\beta_i| $$\n",
    "\n",
    "Donde:\n",
    "\n",
    "* $J(\\beta)$ es la función de costo.\n",
    "* $\\text{Error}$ es una medida del error entre los valores predichos y los valores reales (por ejemplo, la suma de los errores al cuadrado en la regresión lineal estándar).\n",
    "* $\\lambda$ (lambda) es el **parámetro de regularización** (un hiperparámetro que se ajusta). Controla la fuerza de la penalización. Cuanto mayor sea el valor de $\\lambda$, mayor será la penalización.\n",
    "* $\\sum_{i=1}^{p} |\\beta_i|$ es la **norma L1** de los coeficientes, que es la suma de los valores absolutos de todos los coeficientes ($\\beta_i$).\n",
    "* $p$ es el número de variables predictoras.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **La clave de la regresión LASSO: Selección de características** </span>\n",
    "\n",
    "La principal característica distintiva de la regularización L1 (y por lo tanto de la regresión LASSO) es su capacidad para **forzar algunos de los coeficientes de las variables predictoras a ser exactamente cero**.\n",
    "\n",
    "* Cuando $\\lambda$ es suficientemente grande, la penalización L1 puede hacer que los coeficientes de las variables menos importantes se reduzcan a cero, eliminando efectivamente esas variables del modelo.\n",
    "* Esto convierte a la regresión LASSO en un método útil para la **selección de características**, ya que identifica automáticamente las variables más relevantes para la predicción.\n",
    "* El modelo resultante es más **parsimonioso** (tiene menos variables) y, por lo tanto, a menudo más fácil de interpretar.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Diferencias Clave entre Regresión LASSO (L1) y Regresión Ridge (L2)** </span>\n",
    "\n",
    "| Característica          | Regresión LASSO (L1)                                      | Regresión Ridge (L2)                                           |\n",
    "|-------------------------|-----------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Tipo de Regularización** | Norma L1: Suma del valor absoluto de los coeficientes ($|\\beta_i|$) | Norma L2: Suma del cuadrado de los coeficientes ($\\beta_i^2$) |\n",
    "| **Efecto en Coeficientes** | Puede reducir algunos coeficientes a **exactamente cero**.  | Reduce la magnitud de los coeficientes, pero **raramente a cero**. |\n",
    "| **Selección de Características** | **Realiza selección de características** al eliminar variables. | **No realiza una selección de características explícita**.      |\n",
    "| **Parsimonia del Modelo** | Tiende a generar modelos **más parsimoniosos** (menos variables). | Tiende a mantener todas las variables en el modelo (aunque con pesos pequeños). |\n",
    "| **Interpretabilidad** | Puede mejorar la **interpretabilidad** al simplificar el modelo. | La interpretabilidad puede ser menor debido a la presencia de todas las variables. |\n",
    "| **Manejo de Multicolinealidad** | Tiende a seleccionar una variable de un grupo correlacionado. | Distribuye el peso entre las variables correlacionadas.          |\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Ventajas de la regresión LASSO:** </span>\n",
    "\n",
    "* Selección de características automática: Identifica y elimina variables irrelevantes, simplificando el modelo.\n",
    "* Ayuda a prevenir el sobreajuste: Reduce la complejidad del modelo al penalizar los coeficientes grandes.\n",
    "* Útil en conjuntos de datos de alta dimensión: Funciona bien cuando hay muchas variables predictoras, algunas de las cuales pueden ser irrelevantes.\n",
    "* Mejora la interpretabilidad del modelo: Al tener menos variables, el modelo es más fácil de entender.\n",
    "* Puede manejar cierta multicolinealidad: Tiende a seleccionar una variable de un grupo de variables altamente correlacionadas y a establecer los coeficientes de las otras en cero.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Desventajas de la regresión LASSO:** </span>\n",
    "\n",
    "* Puede descartar variables relevantes: Si $\\lambda$ es demasiado grande, podría eliminar variables que realmente tienen un impacto en la predicción.\n",
    "* Selección arbitraria en alta multicolinealidad: Si hay grupos de variables muy correlacionadas, LASSO puede seleccionar una de ellas arbitrariamente, lo que puede ser inestable.\n",
    "* Puede no funcionar tan bien como Ridge si todas las variables son relevantes: Si la mayoría de las variables tienen algún impacto en la predicción, la regresión Ridge podría dar mejores resultados en términos de precisión predictiva.\n",
    "* La elección del parámetro $\\lambda$ es crucial: Un valor incorrecto de $\\lambda$ puede llevar a un modelo subajustado (si $\\lambda$ es demasiado grande) o sobreajustado (si $\\lambda$ es demasiado pequeño). La selección óptima de $\\lambda$ a menudo se realiza mediante técnicas de validación cruzada.\n",
    "\n",
    "En resumen, el algoritmo de regresión LASSO (L1) es una poderosa herramienta para construir modelos lineales más robustos e interpretables, especialmente en situaciones con muchas variables predictoras donde se sospecha que algunas de ellas son irrelevantes. Su capacidad para realizar la selección de características de forma inherente lo convierte en una técnica valiosa en el campo del aprendizaje automático y la estadística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <span style=\"color:#ffc509\">  **Links útiles** </span>\n",
    "\n",
    "https://www.ibm.com/es-es/think/topics/lasso-regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
