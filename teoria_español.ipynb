{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# <span style=\"color:#ffc509;\"> **Regresión LASSO L1** (Operador de Selección y Contracción Menor Absolutor) </span>\n",
    "\n",
    "</div>\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión Lasso es una técnica estadística utilizada en el aprendizaje automático para mejorar la precisión de los modelos predictivos, especialmente cuando se trabaja con grandes conjuntos de datos. Su principal objetivo es prevenir el sobreajuste, un problema que ocurre cuando un modelo se ajusta demasiado a los datos de entrenamiento y no funciona bien con datos nuevos. Lasso lo logra mediante la aplicación de una penalización que reduce la magnitud de los coeficientes de las variables predictoras, incluso forzando algunos a cero. Esto simplifica el modelo, selecciona automáticamente las variables más importantes y mejora su capacidad de generalización.\n",
    "\n",
    "<i>\n",
    "Aclaración:\n",
    "Aunque la regresión Lasso y la regularización L1 suelen mencionarse como si fueran lo mismo, en realidad no lo son. Lasso es un modelo de regresión que utiliza la penalización L1, pero la regularización L1 también se emplea en otros modelos más allá de la regresión, como en regresión logística, máquinas de soporte vectorial (SVM), redes neuronales y selección de características en modelos lineales. La confusión ocurre porque Lasso es el ejemplo más común de regularización L1, lo que lleva a que muchas fuentes los traten como sinónimos. </i>\n",
    "\n",
    "### <span style=\"color:#ffc509\"> ¿Qué es la **regularización?** </span>\n",
    "\n",
    "En el aprendizaje automático, la regularización es como darle un freno al modelo para que no se vuelva demasiado complicado. El problema es que, a veces, el modelo se aprende de memoria los datos de entrenamiento, como un estudiante que memoriza respuestas exactas para un examen. Esto se llama \"sobreajuste\". Cuando esto pasa, el modelo funciona mal con datos nuevos. La regularización añade una \"penalización\" a la formula que el modelo usa para saber cuando se equivoca, para que aprenda de manera más general, y no tan especificamente, y de esta manera, no se memorice los datos de entrenamiento.\n",
    "\n",
    "### <span style=\"color:#ffc509\"> **¿Qué** es la regresión Lasso? </span>\n",
    "\n",
    "La regresión Lasso, también conocida como regularización L1, es un tipo de regresión lineal que añade una penalización basada en el valor absoluto de los coeficientes de las variables predictoras. Esta penalización fuerza a algunos coeficientes a ser exactamente cero, lo que significa que esas variables se eliminan del modelo.\n",
    "\n",
    "### <span style=\"color:#ffc509\"> **¿Por qué** se utiliza regresión Lasso? </span>\n",
    "\n",
    "- Prevención del sobreajuste: Reduce la complejidad del modelo, evitando que se ajuste demasiado a los datos de entrenamiento.\n",
    "- Selección automática de variables: Identifica y retiene solo las variables más relevantes para la predicción, simplificando el modelo y mejorando su interpretabilidad.\n",
    "- Manejo de multicolinealidad: Aunque no la elimina por completo, Lasso puede mitigar los efectos de la multicolinealidad (alta correlación entre variables predictoras) al seleccionar una variable y eliminar las correlacionadas\n",
    "\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **¿Cómo** funciona la regresión LASSO? </span>\n",
    "\n",
    "1.  Penalización L1: Lasso añade un término de penalización a la función de coste del modelo de regresión lineal. Este término es proporcional a la suma de los valores absolutos de los coeficientes de las variables predictoras.\n",
    "2. Parámetro Lambda (λ): Este parámetro controla la fuerza de la penalización.\n",
    "    - Un λ alto aumenta la penalización, lo que reduce más coeficientes a cero, simplificando el modelo.\n",
    "    - Un λ bajo disminuye la penalización, conservando más variables en el modelo.\n",
    "3. Minimización del Error Cuadrático Medio (MSE): Lasso busca el valor de los coeficientes que minimiza el MSE (la diferencia entre los valores predichos y los reales), sujeto a la penalización L1.\n",
    "4. Selección de Variables: Al forzar algunos coeficientes a cero, Lasso realiza una selección automática de variables, quedándose solo con las más importantes.\n",
    "\n",
    "\n",
    "La función de costo que la regresión LASSO trata de minimizar es:\n",
    "\n",
    "$$ J(\\beta) = \\text{Error} + \\lambda \\sum_{i=1}^{p} |\\beta_i| $$\n",
    "\n",
    "Donde:\n",
    "\n",
    "* $J(\\beta)$ es la función de costo.\n",
    "* $\\text{Error}$ es una medida del error entre los valores predichos y los valores reales (por ejemplo, la suma de los errores al cuadrado en la regresión lineal estándar).\n",
    "* $\\lambda$ (lambda) es el **parámetro de regularización** (un hiperparámetro que se ajusta). Controla la fuerza de la penalización. Cuanto mayor sea el valor de $\\lambda$, mayor será la penalización.\n",
    "* $\\sum_{i=1}^{p} |\\beta_i|$ es la **norma L1** de los coeficientes, que es la suma de los valores absolutos de todos los coeficientes ($\\beta_i$).\n",
    "* $p$ es el número de variables predictoras.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Cuándo** se utiliza la regresión Lasso? </span>\n",
    "\n",
    "- Conjuntos de datos de alta dimensión: Cuando hay muchas más variables predictoras que observaciones.\n",
    "- Selección automática de variables: Cuando se necesita identificar las variables más relevantes para la predicción.\n",
    "- Problemas predictivos: Donde el objetivo principal es la precisión de la predicción.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **La clave de la regresión LASSO: Selección de características** </span>\n",
    "\n",
    "La principal característica distintiva de la regularización L1 (y por lo tanto de la regresión LASSO) es su capacidad para **forzar algunos de los coeficientes de las variables predictoras a ser exactamente cero**.\n",
    "\n",
    "* Cuando $\\lambda$ es suficientemente grande, la penalización L1 puede hacer que los coeficientes de las variables menos importantes se reduzcan a cero, eliminando efectivamente esas variables del modelo.\n",
    "* Esto convierte a la regresión LASSO en un método útil para la **selección de características**, ya que identifica automáticamente las variables más relevantes para la predicción.\n",
    "* El modelo resultante es más **parsimonioso** (tiene menos variables) y, por lo tanto, a menudo más fácil de interpretar.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Diferencias Clave entre Regresión LASSO (L1) y Regresión Ridge (L2)** </span>\n",
    "\n",
    "| Característica          | Regresión LASSO (L1)                                      | Regresión Ridge (L2)                                           |\n",
    "|-------------------------|-----------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Tipo de Regularización** | Norma L1: Suma del valor absoluto de los coeficientes ($|\\beta_i|$) | Norma L2: Suma del cuadrado de los coeficientes ($\\beta_i^2$) |\n",
    "| **Efecto en Coeficientes** | Puede reducir algunos coeficientes a **exactamente cero**.  | Reduce la magnitud de los coeficientes, pero **raramente a cero**. |\n",
    "| **Selección de Características** | **Realiza selección de características** al eliminar variables. | **No realiza una selección de características explícita**.      |\n",
    "| **Parsimonia del Modelo** | Tiende a generar modelos **más parsimoniosos** (menos variables). | Tiende a mantener todas las variables en el modelo (aunque con pesos pequeños). |\n",
    "| **Interpretabilidad** | Puede mejorar la **interpretabilidad** al simplificar el modelo. | La interpretabilidad puede ser menor debido a la presencia de todas las variables. |\n",
    "| **Manejo de Multicolinealidad** | Tiende a seleccionar una variable de un grupo correlacionado. | Distribuye el peso entre las variables correlacionadas.          |\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Ventajas** de la regresión LASSO: </span>\n",
    "\n",
    "* Selección de características automática: IIdentifica y elimina variables irrelevantes, simplificando el modelo y facilitando su interpretación. Por ejemplo, al predecir el precio de una casa, LASSO selecciona las características más relevantes como el tamaño y la ubicación.\n",
    "* Ayuda a prevenir el sobreajuste: Reduce la complejidad del modelo al penalizar los coeficientes grandes, lo que mejora la precisión en datos nuevos.\n",
    "* Útil en conjuntos de datos de alta dimensión: Funciona bien cuando hay muchas variables predictoras (alta dimensión), algunas de las cuales pueden ser irrelevantes.\n",
    "* Mejora la interpretabilidad del modelo: Al tener menos variables, el modelo es más fácil de entender y explicar.\n",
    "* Puede manejar cierta multicolinealidad: Tiende a seleccionar una variable de un grupo de variables altamente correlacionadas y a establecer los coeficientes de las otras en cero. Por ejemplo, si tienes variables como \"metros cuadrados\" y \"número de habitaciones\", LASSO selecciona la más importante.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Desventajas** de la regresión LASSO: </span>\n",
    "\n",
    "* Puede descartar variables relevantes: Si $\\lambda$ es demasiado grande,  podría eliminar variables que realmente tienen un impacto en la predicción, lo que reduciría la precisión del modelo.\n",
    "* Selección arbitraria en alta multicolinealidad: Si hay grupos de variables muy correlacionadas, LASSO puede seleccionar una de ellas arbitrariamente(al azar), lo que puede ser inestable, ya que pequeñas variaciones en los datos podrían cambiar la variable seleccionada.\n",
    "* Puede no funcionar tan bien como Ridge si todas las variables son relevantes: Si la mayoría de las variables tienen algún impacto en la predicción, la regresión Ridge podría dar mejores resultados en términos de precisión predictiva.\n",
    "* La elección del parámetro $\\lambda$ es crucial: Un valor incorrecto de $\\lambda$ puede llevar a un modelo subajustado (si $\\lambda$ es demasiado grande) o sobreajustado (si $\\lambda$ es demasiado pequeño). La selección óptima de $\\lambda$ a menudo se realiza mediante técnicas de validación cruzada.\n",
    "\n",
    "\n",
    "### <span style=\"color:#ffc509\"> Casos de Uso Comunes </span>\n",
    "\n",
    "- Genómica: Identificación de genes relevantes para ciertas enfermedades.\n",
    "- Marketing: Selección de las variables demográficas y de comportamiento más importantes para predecir la respuesta a una campaña publicitaria.\n",
    "- Finanzas: Construcción de modelos de riesgo crediticio, seleccionando los factores más influyentes.\n",
    "\n",
    "### <span style=\"color:#ffc509\"> **Diccionario** de Términos Clave </span>\n",
    "\n",
    "- **Sobreajuste** (**Overfitting**): Un modelo que se ajusta demasiado a los datos de entrenamiento y no generaliza bien a datos nuevos.\n",
    "- **Regularización**: Técnicas para prevenir el sobreajuste, añadiendo información adicional para evitar la complejidad excesiva del modelo.\n",
    "- **Penalización L1**: La penalización utilizada en Lasso, basada en el valor absoluto de los coeficientes.\n",
    "- **Lambda** (**λ**): El parámetro que controla la fuerza de la penalización en Lasso.\n",
    "- **Alfa** (**α**): Este término no se menciona en el texto proporcionado.\n",
    "- **Multicolinealidad**: Alta correlación entre variables predictoras.\n",
    "- **Error Cuadrático Medio** (**MSE**): Una medida de la diferencia entre los valores predichos y los reales.\n",
    "- **Sesgo**: La diferencia entre las predicciones promedio de un modelo y los valores reales.\n",
    "- **Varianza**: La variabilidad de las predicciones de un modelo para diferentes conjuntos de datos.\n",
    "- **Función de Coste**: Una función que mide qué tan bien un modelo se ajusta a los datos; el objetivo es minimizar esta función.\n",
    "- **Coeficientes Beta** (**β**): Los valores que multiplican las variables predictoras en un modelo de regresión.\n",
    "- **R2**: Una medida de qué proporción de la varianza en la variable dependiente es explicada por las variables independientes.\n",
    "- **Variable Dependiente**: La variable que se está prediciendo.\n",
    "- **Variables Independientes**: Las variables utilizadas para hacer predicciones.\n",
    "- **Elastic Net**: Una técnica de regularización que combina las penalizaciones L1 (Lasso) y L2 (Ridge).\n",
    "- **Ridge**: También conocida como regularización L2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "# <span style=\"color:#ffc509\">  **Ejemplo**: ☕ </span>\n",
    "\n",
    "</div>\n",
    "\n",
    "### Tenemos una cafetería y quieremos saber qué cosas hacen que venda más café por día.\n",
    "\n",
    "Hemos estado anotando datos durante varias semanas, y tenemos variables como:\n",
    "\n",
    "- ✅ Hace frío\n",
    "- ✅ Es fin de semana\n",
    "- ✅ Hay promociones\n",
    "- ✅ Hay música en vivo\n",
    "- ✅ Se encendieron luces decorativas\n",
    "- ✅ Cambio en el menú\n",
    "- ✅ Hay sillas nuevas\n",
    "- ✅ Hubo luna llena 🌕\n",
    "- ✅ El gato se subió al mostrador 😺\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Nuestro objetivo:\n",
    "**Predecir cuántos cafés se venderán**, dependiendo de nuestros factores factores.\n",
    "\n",
    "Pero hay un problema:\n",
    "\n",
    "👉 No todos esos factores realmente influyen.\n",
    "Algunos son importantes (como promociones), otros son ruido (como la luna llena o el gato 😺).\n",
    "\n",
    "---\n",
    "\n",
    "### 📉 Aquí entra la regresión Lasso (L1)\n",
    "\n",
    "Lasso es un tipo de **regresión lineal con regularización**.\n",
    "Es como un filtro que te ayuda a:\n",
    "\n",
    "- 🧠 Seleccionar solo las variables que realmente ayudan a predecir\n",
    "- 🧹 Eliminar o ignorar las que no sirven\n",
    "- 🧾 Hacer el modelo más simple y más generalizable\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 ¿Cómo funciona en la cafetería?\n",
    "\n",
    "Digamos que alimentamos al modelo con registros de muchos días:\n",
    "\n",
    "- Día 1: Hace frío + fin de semana + promoción + música = 280 cafés\n",
    "- Día 2: Calor + lunes + sin promo + gato en el mostrador = 90 cafés\n",
    "- Día 3: Frío + cambio de menú + luces + música = 200 cafés\n",
    "- Día 4: Luna llena + sillas nuevas + promo = 100 cafés\n",
    "\n",
    "El modelo va **aprendiendo qué cosas realmente impactan en las ventas**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 Modelo clásico vs. Lasso\n",
    "\n",
    "### 🧮 Modelo de regresión lineal (sin penalización):\n",
    "$$ \\hat{y} = \\beta_0 + \\beta_1 \\cdot \\text{Frío} + \\beta_2 \\cdot \\text{FinDeSemana} + \\beta_3 \\cdot \\text{Promoción} + \\beta_4 \\cdot \\text{Música} + \\beta_5 \\cdot \\text{Luces} + \\beta_6 \\cdot \\text{Menú} + \\beta_7 \\cdot \\text{Sillas} + \\beta_8 \\cdot \\text{LunaLlena} + \\beta_9 \\cdot \\text{Gato} $$\n",
    "\n",
    "$\\hat{y}$ corresponde a las ventas predichas.\n",
    "\n",
    "Este modelo podría dar pesos ($\\beta_i$) pequeños pero no exactamente cero a variables irrelevantes como la luna llena o el gato.\n",
    "\n",
    "### 🪄 Regresión Lasso (con **penalización L1**):\n",
    "\n",
    "$$ \\min_{\\beta} \\left\\{ \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\} $$\n",
    "\n",
    "\n",
    "que sería: \n",
    "\n",
    "$$ \\text{Coste} = \\underbrace{\\frac{1}{2n} \\sum_{i=1}^{n} (\\text{Ventas}_i - \\text{VentasPredichas}_i)^2}_{\\text{Qué tan lejos están nuestras predicciones de las ventas reales}} + \\underbrace{\\lambda \\sum_{j=1}^{p} |\\beta_j|}_{\\text{Castigo por dar importancia a demasiados factores}} $$\n",
    "\n",
    "\n",
    "$ \\min_{\\beta}$ = El valor que queremos minimizar\n",
    "\n",
    "\n",
    "$ \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} x_{ij} \\beta_j)^2 $ = **SSE** (suma de errores cuadráticos) Suma los términos que le siguen para cada observación $_i$, desde la primera ($_i$=1) hasta la última ($_i$=n).\n",
    "\n",
    "- $y_i $ = El número real de cafés vendidos en el día $_i$\n",
    "- $\\beta_0 $ = El número que el modelo predice para el día $_i$\n",
    "\n",
    "- $\\sum_{j=1}^{p} x_{ij} \\beta_j$ = contribución de todas las variables donde para una observación específica $_i$ , multiplica el valor de cada variable predictora ${j}$ $x_{ij}$ por su correspondiente coeficiente $\\beta_j$ y luego suma todos estos productos para todas las $^{p}$ variables predictoras.\n",
    "\n",
    "    - $\\sum$ = sumatoria\n",
    "    - ${}^{p}_{j=1}$  = Son los límites de la sumatoria\n",
    "        - ${j}$ = índice de la sumatoria\n",
    "        - =1 = indica que la sumatoria comienza con el valor de ${j}$ igual a 1 (primera variable predictora del conjunto de datos)\n",
    "        - $^{p}$ = indica que la sumatoria termina cuando el valor de ${j}$ alcanza ${p}$ . En donde ${p}$ representa el número total de variables predictoras en el modelo\n",
    "    - $x_{ij}$ = representa el valor de la ${j}$ -ésima variable predictora para la ${i}$ -ésima observación\n",
    "    - $\\beta_j$ = beta con subíndice ${j}$ representa el coeficiente asociado a la ${j}$ -ésima variable predictora.\n",
    "    - $^{2}$ = el error es elevado al cuadrado para: 1.- evitar la cancelación de errores positivos y negativos. 2.- Penaliza más los errores grandes. 3.- Facilita el cálculo\n",
    "\n",
    "$\\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $ = **MSE** (error cuadrático medio). Cómo la SSE pero añadiendo el factor escala que mide qué tan lejos están nuestras predicciones de los valores reales\n",
    "\n",
    "- $y_i$ = Valor real de la variable dependiente (números de cafés vendidos)\n",
    "- $\\hat{y}_i$ = Valor predicho de la variable dependiente por el modelo de regresión para la $_i$ ésima observación (números de cafés que el modelo predice que se venderán el día $_i$)\n",
    "- $^{2}$ = El error es elevado al cuadrado para: 1.- evitar la cancelación de errores positivos y negativos. 2.- Penaliza más los errores grandes. 3.- Facilita el cálculo\n",
    "\n",
    "$\\lambda \\sum_{j=1}^{p} |\\beta_j|$ = es la **penalización L1**. Esta penalización fuerza a los coeficientes de las variables menos importantes a ser exactamente **cero**.\n",
    "- $\\lambda$ = Parámetro de regularizaciónn o constante de penalización\n",
    "    - Si es cercano a 0, la penalización es débil (sin mucha tendencia a reducir los coeficientes a cero)\n",
    "    - Si es grande, la penalización es fuerte (modelo incentivado a hacer muchos de los coeficientes de las variables predictoras cero)\n",
    "    - Valor óptimo : Se encuentra generalmente mediante técnicas de validación cruzada.\n",
    "- $\\sum$ = sumatoria\n",
    "    - ${}^{p}_{j=1}$  = Son los límites de la sumatoria\n",
    "        - ${j}$ = índice de la sumatoria\n",
    "        - =1 = indica que la sumatoria comienza con el valor de ${j}$ igual a 1 (primera variable predictora del conjunto de datos)\n",
    "        - $^{p}$ = indica que la sumatoria termina cuando el valor de ${j}$ alcanza ${p}$ . En donde ${p}$ representa el número total de variables predictoras en el modelo\n",
    "- $\\beta_j$ = Coeficiente (el peso) asociado con la ${j}$ -ésima variable predictora del modelo. Indica la fuerza y dirección de la relación entre las variables predictora y dependiente (impacto considerable o no en la venta de café).\n",
    "- $|\\cdot|$ = Símbolo de valor absoluto, es decir, se toma el valor (número) absoluto independientemente de si es negativo o positivo (-3=3, 3=3)\n",
    "---\n",
    "\n",
    "### ✨ El resultado mágico:\n",
    "\n",
    "Después de entrenar el modelo Lasso con los datos, podríamos obtener algo como:\n",
    "\n",
    "$$ \\text{Ventas} \\approx 50 + 80 \\cdot \\text{Frío} + 120 \\cdot \\text{FinDeSemana} + 150 \\cdot \\text{Promoción} + 60 \\cdot \\text{Música} + 0 \\cdot \\text{Luces} + 30 \\cdot \\text{Menú} + 0 \\cdot \\text{Sillas} + 0 \\cdot \\text{LunaLlena} + 0 \\cdot \\text{Gato} $$\n",
    "\n",
    "Y esto nos dice claramente que:\n",
    "\n",
    "- 🌡️ El **frío**, los **fines de semana** y las **promociones** son factores clave que aumentan tus ventas.\n",
    "- 🎶 La **música en vivo** y un **nuevo menú** tienen un impacto menor pero positivo.\n",
    "- 💡 Las **luces decorativas**, las **sillas nuevas**, la **luna llena** y las travesuras de tu **gato** parecen no tener una influencia significativa en las ventas.\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 En resumen, Lasso nos ayuda a:\n",
    "\n",
    "- **Identificar los factores de venta reales.**\n",
    "- **Simplificar tu modelo de predicción.**\n",
    "- **Tomar decisiones más inteligentes para tu cafetería.**\n",
    "\n",
    "con esto, ahora sí nos podemos enfocar en los factores que realmente influyen en nuestro negocio haciéndolo crecer! ☕\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <span style=\"color:#ffc509\">  **Links útiles** </span>\n",
    "\n",
    "https://www.ibm.com/es-es/think/topics/lasso-regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
