{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# <span style=\"color:#ffc509;\"> **Regresión Lasso L1** (Least Absolute Shrinkage and Selection Operator) </span>\n",
    "\n",
    "</div>\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de regresión LASSO (Least Absolute Shrinkage and Selection Operator), es un algoritmo que utiliza la regularización L1.\n",
    "L1 es la técnica de penalización, y LASSO es el modelo que la aplica..\n",
    "Ejemplo: L1 es como una regla, y el Lasso es un jugador que usa la regla.\n",
    "\n",
    "### <span style=\"color:#ffc509\"> **¿Qué es la regularización?** </span>\n",
    "\n",
    "En el aprendizaje automático, la regularización es como darle un freno al modelo para que no se vuelva demasiado complicado. El problema es que, a veces, el modelo se aprende de memoria los datos de entrenamiento, como un estudiante que memoriza respuestas exactas para un examen. Esto se llama \"sobreajuste\". Cuando esto pasa, el modelo funciona mal con datos nuevos. La regularización añade una \"penalización\" a la formula que el modelo usa para saber cuando se equivoca, para que aprenda de manera más general, y no tan especificamente, y de esta manera, no se memorice los datos de entrenamiento.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **¿Cómo funciona la regresión LASSO ?** </span>\n",
    "\n",
    "La regresión LASSO utiliza la **regularización L1**. Esto implica que se introduce una penalización en la función de costo, proporcional a la **suma del valor absoluto** de los coeficientes de las variables predictoras\n",
    "\n",
    "Penalización L1 (LASSO): La penalización es la suma de los valores absolutos de los coeficientes. Esta forma de penalización tiene la propiedad de poder llevar algunos coeficientes a exactamente cero, realizando así una selección de características.\n",
    "\n",
    "La función de costo que la regresión LASSO trata de minimizar es:\n",
    "\n",
    "$$ J(\\beta) = \\text{Error} + \\lambda \\sum_{i=1}^{p} |\\beta_i| $$\n",
    "\n",
    "Donde:\n",
    "\n",
    "* $J(\\beta)$ es la función de costo.\n",
    "* $\\text{Error}$ es una medida del error entre los valores predichos y los valores reales (por ejemplo, la suma de los errores al cuadrado en la regresión lineal estándar).\n",
    "* $\\lambda$ (lambda) es el **parámetro de regularización** (un hiperparámetro que se ajusta). Controla la fuerza de la penalización. Cuanto mayor sea el valor de $\\lambda$, mayor será la penalización.\n",
    "* $\\sum_{i=1}^{p} |\\beta_i|$ es la **norma L1** de los coeficientes, que es la suma de los valores absolutos de todos los coeficientes ($\\beta_i$).\n",
    "* $p$ es el número de variables predictoras.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **La clave de la regresión LASSO: Selección de características** </span>\n",
    "\n",
    "La principal característica distintiva de la regularización L1 (y por lo tanto de la regresión LASSO) es su capacidad para **forzar algunos de los coeficientes de las variables predictoras a ser exactamente cero**.\n",
    "\n",
    "* Cuando $\\lambda$ es suficientemente grande, la penalización L1 puede hacer que los coeficientes de las variables menos importantes se reduzcan a cero, eliminando efectivamente esas variables del modelo.\n",
    "* Esto convierte a la regresión LASSO en un método útil para la **selección de características**, ya que identifica automáticamente las variables más relevantes para la predicción.\n",
    "* El modelo resultante es más **parsimonioso** (tiene menos variables) y, por lo tanto, a menudo más fácil de interpretar.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Diferencias Clave entre Regresión LASSO (L1) y Regresión Ridge (L2)** </span>\n",
    "\n",
    "| Característica          | Regresión LASSO (L1)                                      | Regresión Ridge (L2)                                           |\n",
    "|-------------------------|-----------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Tipo de Regularización** | Norma L1: Suma del valor absoluto de los coeficientes ($|\\beta_i|$) | Norma L2: Suma del cuadrado de los coeficientes ($\\beta_i^2$) |\n",
    "| **Efecto en Coeficientes** | Puede reducir algunos coeficientes a **exactamente cero**.  | Reduce la magnitud de los coeficientes, pero **raramente a cero**. |\n",
    "| **Selección de Características** | **Realiza selección de características** al eliminar variables. | **No realiza una selección de características explícita**.      |\n",
    "| **Parsimonia del Modelo** | Tiende a generar modelos **más parsimoniosos** (menos variables). | Tiende a mantener todas las variables en el modelo (aunque con pesos pequeños). |\n",
    "| **Interpretabilidad** | Puede mejorar la **interpretabilidad** al simplificar el modelo. | La interpretabilidad puede ser menor debido a la presencia de todas las variables. |\n",
    "| **Manejo de Multicolinealidad** | Tiende a seleccionar una variable de un grupo correlacionado. | Distribuye el peso entre las variables correlacionadas.          |\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Ventajas de la regresión LASSO:** </span>\n",
    "\n",
    "* Selección de características automática: IIdentifica y elimina variables irrelevantes, simplificando el modelo y facilitando su interpretación. Por ejemplo, al predecir el precio de una casa, LASSO selecciona las características más relevantes como el tamaño y la ubicación.\n",
    "* Ayuda a prevenir el sobreajuste: Reduce la complejidad del modelo al penalizar los coeficientes grandes, lo que mejora la precisión en datos nuevos.\n",
    "* Útil en conjuntos de datos de alta dimensión: Funciona bien cuando hay muchas variables predictoras (alta dimensión), algunas de las cuales pueden ser irrelevantes.\n",
    "* Mejora la interpretabilidad del modelo: Al tener menos variables, el modelo es más fácil de entender y explicar.\n",
    "* Puede manejar cierta multicolinealidad: Tiende a seleccionar una variable de un grupo de variables altamente correlacionadas y a establecer los coeficientes de las otras en cero. Por ejemplo, si tienes variables como \"metros cuadrados\" y \"número de habitaciones\", LASSO selecciona la más importante.\n",
    "\n",
    "### <span style=\"color:#ffc509\">  **Desventajas de la regresión LASSO:** </span>\n",
    "\n",
    "* Puede descartar variables relevantes: Si $\\lambda$ es demasiado grande,  podría eliminar variables que realmente tienen un impacto en la predicción, lo que reduciría la precisión del modelo.\n",
    "* Selección arbitraria en alta multicolinealidad: Si hay grupos de variables muy correlacionadas, LASSO puede seleccionar una de ellas arbitrariamente(al azar), lo que puede ser inestable, ya que pequeñas variaciones en los datos podrían cambiar la variable seleccionada.\n",
    "* Puede no funcionar tan bien como Ridge si todas las variables son relevantes: Si la mayoría de las variables tienen algún impacto en la predicción, la regresión Ridge podría dar mejores resultados en términos de precisión predictiva.\n",
    "* La elección del parámetro $\\lambda$ es crucial: Un valor incorrecto de $\\lambda$ puede llevar a un modelo subajustado (si $\\lambda$ es demasiado grande) o sobreajustado (si $\\lambda$ es demasiado pequeño). La selección óptima de $\\lambda$ a menudo se realiza mediante técnicas de validación cruzada.\n",
    "\n",
    "En resumen, el algoritmo de regresión LASSO es una poderosa herramienta para construir modelos lineales robustos e interpretables. Este algoritmo utiliza la regularización L1, una técnica que penaliza la suma de los valores absolutos de los coeficientes, para realizar tanto la selección de características como la regularización. Esto lo hace especialmente útil en situaciones con muchas variables predictoras, donde se sospecha que algunas de ellas son irrelevantes. Su capacidad inherente para realizar la selección de características simplifica los modelos y mejora la precisión predictiva. Sin embargo, requiere una cuidadosa selección del parámetro λ y puede tener limitaciones con variables altamente correlacionadas. A pesar de estas limitaciones, LASSO, al aplicar la regularización L1, sigue siendo una técnica valiosa en el campo del aprendizaje automático y la estadística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <span style=\"color:#ffc509\">  **Links útiles** </span>\n",
    "\n",
    "https://www.ibm.com/es-es/think/topics/lasso-regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
